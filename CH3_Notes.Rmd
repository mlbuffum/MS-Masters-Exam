---
title: "Design of Experiment"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(dplyr)
library(reshape2)
library(knitr)
library(ggplot2)
```
# 3 Experiments with a Single-Factor: The Analysis of Variance

## 3.1 An Example
In many integrated manufacturing steps, wafers are completed coated with a layer of material such as silicon dioxide or a metal. The unwanted material is then selectively removed by etching through a mask, thereby creating circuit patterns, electrical interconnects, and areas in which diffusions are to be made. A plasma etching process is widely used for this operation, particularly in small geometry applications. Energy s supplied by a radio-frequency (RF) generator causing plasma to be generated in the gap between the elecctrodes. The chemical species in the plasma are determined b the particular gases used. Fluorocarbons, such as CF_4 or C_2F_6, are often used in plasma etching, but other gases and mixtures of gases are relatively common, depending on the application.
  
An engineer is interested in investifating the relationship between the RF power setting and the etch rate for this tool. The objective of an experiment like this is to model the relationship between etch rate and RF power, and to specify the power setting that will give a desired target etch rate. She is interested in a particular gas (C_2F_6) and gap (0.80 cm) and wants to test four levels of RF power: 160, 180, 200, and 220 W. She decided to test five wafers at each level of RF power.

```{r, echo = F}

dat0 <- as.data.frame(array(
  c(
    rep(c(160,180,200,220),5),
    rep(1,4),rep(2,4),rep(3,4),rep(4,4),rep(5,4),
    575,565,600,725,
    542,593,651,700,
    530,590,610,715,
    539,579,637,685,
    570,610,629,710
  ),
  dim = c(20,3)
))

colnames(dat0) <- c("Power","Replication","EtchRate")

# Check Totals
check_rowTotals <- summarize(
  group_by(dat0,Power),
  rowTot = sum(EtchRate),
  rowAvg = mean(EtchRate)
)

# Make power a factor
dat0$Power <- as.factor(dat0$Power)

```
  
Examine the data graphically:
```{r, echo = F}
boxplot(EtchRate ~ Power, data = dat0)
```
  
Suppose we wish to test for differences between the mean etch rate at all $\alpha = 4$ levels of RF power (and thus, the equality of all four means). We should NOT perform all six pairwise t-tests:
  
* Performing all six pairwise t-tests is inefficient
* Inflates type 1 error: suppose that all four means are equal, so if we select $\alpha = 0.05$, the probability of reaching the correct decision on any single comparison is 0.95. However, the probability of reach the correct conclusion on all six comparisons is considerably less than 0.95, so Type I Error is inflated.
  
Need to use the **Analysis of Variance**.
  
## 3.2 The Analysis of Variance
Suppose we have *a* treatments, or different levels, of a single factor that we wish to compare. A response $y_{ij}$ represents the $j^{th}$ observation taken under that factor level or treatment $i$. There will be *n* observations under the $i^{th}$ treatment.
  
##### Models for the Data
\[
y_{ij} = \mu_i + \epsilon_{ij}
\]
  
where
  
\[
\begin{aligned}
i &= 1,2,...,a\\
j &= 1,2,...,n
\end{aligned}
\]
  
The **means model** is an alternative way to write a model for the data:
\[
\mu_{i} = \mu + \tau_i
\]
  
so that the **effects model** is
\[
y_{ij} = \mu + \tau_i + \epsilon_{ij}
\]
  
Therefore, we have
  
* $\mu$ is the **overall mean**
* $\tau_i$ is the **ith treatment effect**
  
This is called the **one-way** or **single-factor ANOVA** model. Because runs were conducted in a random order, the experimental design is a **completely randomized design**. We assume that:
\[
y_{ij} \sim N(\mu + \tau_i,\sigma^2)
\]
  
##### Fixed or Random Factor
The statistical model above describes two different situations with respect to the treatment effects:
  
1. **Fixed Effects Model.** The *a* treatmens could have been specifically chosen by the experimenter. In this case, we wish to test the hypotheses about the treatment means, and our conclusion will only apply to the factor levels considered in the analysis.
2. **Random Effects Model.** The *a* treatments could a random sample from a larger population of treatments. In this case, we wish to extend the conclusion to all treatments in the population, regardless of their inclusion in the analysis. Here, the $\tau_i$ are random variables. We test hypotheses about the variability of the $\tau_i$ (**components of variance model**).
  
#### Analysis of the Fixed Effects Model
Recall that:
  
* $y_{i.}$ is the sum of the observations under the ith treatment
* $\bar y_{i.}$ is the mean of the observations under the ith treatment
* $y_{..}$ is the grand total of all observations
* $\bar y_{..}$ is the grand mean of all observations
  
We are interested in testing the equality of the *a* treatment means; that is,
\[E(y_{ij})=\mu + \tau_i = \mu_i,\ i=1,2,...,a\]
  
The appropriate hypotheses are:
\[
\begin{aligned}
H_0 :& \mu_1 = \mu_2 = ... = \mu_a\\
H_1 :& \mu_i \ne \mu_j\ \text{for at least one pair }(i,j)
\end{aligned}
\]
  
In the **effects model**, we break the ith treatment mean into two components such that $\mu_i = \mu + \tau_i$. We usually think of $\mu$ as the **overall mean** so that
\[
\mu = \frac{1}{a}\sum_{i=1}^a \mu_i
\]
  
But this definition implies that
\[
\sum_{i=1}^a \tau_i = 0
\]
  
That is, the treatment or factor effects can be thought of as **deviations from the overall mean**. Consequently, another way to write the hypotheses is
\[
\begin{aligned}
H_0 :& \tau_1 = \tau_2 = ... = \tau_a\\
H_1 :& \tau_i \ne 0\ \text{for at least one }i
\end{aligned}
\]
  
### 3.3.1 Decomposition of the Total Sum of Squares
The name **analysis of variance** is derived from a partitioning of total variability into its component parts. The total corrected sum of squares
\[
SS_T = \sum_{i=1}^a \sum_{j=1}^n (y_{ij} - \bar y_{..})^2
\]
  
is used as a measure of overall variability in the data. (**Sample variance** of the *y*'s is the total sum of squares divided by the DF - 1, or $an - 1$, the standard measure of variability.)
  
The total corrected sum of squares may be written as
\[
\begin{aligned}
\sum_{i=1}^a \sum_{j=1}^n (y_{ij} - \bar y_{..})^2 &= \sum_{i=1}^a \sum_{j=1}^n [(\bar y_{i.} - \bar y_{..}) + (y_{ij} - \bar y_{i.})]^2\\
&=n\sum_{i=1}^2 (\bar y_{i.} - \bar y_{..})^2 + \sum_{i=1}^a \sum_{j=1}^n(y_{ij} - \bar y_{i.})^2\\
&+ 2\sum_{i=1}^a \sum_{j=1}^n(\bar y_{i.} - \bar y_{..})(y_{ij} - \bar y_{i.})\\
&=n\sum_{i=1}^2 (\bar y_{i.} - \bar y_{..})^2 + \sum_{i=1}^a \sum_{j=1}^n(y_{ij} - \bar y_{i.})^2
\end{aligned}
\]
  
because the cross-product term equals 0:
\[
\sum_{j=1}^n(y_{ij} - \bar y_{i.}) = \sum_{j=1}^ny_{ij} - \sum_{j=1}^n\bar y_{i.}=y_{i.} - n\bar y_{i.}=0.
\]
  
This is a fundamental ANOVA identity, and states that the total variability in the data can be partitioned into:
  
* a sum of squares **between** levels of treatment (ith mean minus the grand mean), that is, difference in treatment means, and
* a sum of squares **within** each level of treatment (jth observation minus the ith treatment mean), which we assume is due to random error unless otherwise controlled for.
\[
SS_T = SS_{Treatment} + SS_{Error}
\]
  
There are:
  
* $N-1$ degrees of freedom total
* $a-1$ degrees of freedom for treatment
* $N-a = a(n-1)$ degrees of freedom for error (there are $n - 1$ degrees of freedom with which to measure error in each treatment group)
  
##### Sum of Squares Within Treaments ($SS_{Error}$)
Let's look at the error sum of squares within a treatment group, that is, within treatment group $i$:
\[
SS_{Error} = \sum_{i=1}^a\sum_{j=1}^n(y_{ij} - \bar y_{i.})^2 = \sum_{i=1}^a\biggr[\sum_{j=1}^n(y_{ij} - \bar y_{i.})^2\biggl]
\]
  
The component of the sum of squared errors inside the bracket, when divided by $n-1$, is the **sample variance** in the ith treatment, or
\[
S_i^2 = \frac{\sum_{j=1}^n(y_{ij} - \bar y_{i.})^2}{n-1},\ i = 1,2,...,a
\]
  
We have $a$ sample variances, since there are $a$ treatments. These can be combined across levels of treatments to give a single estimate of the common population variance, a **pooled estimate** of the common variance within each of the $a$ treatments:
\[
SS_{pooled} = \frac{\sum_{i=1}^a\biggr[\sum_{j=1}^n(y_{ij} - \bar y_{i.})^2\biggl]}{\sum_{i=1}^a(n-1)} = \frac{SS_{Error}}{N-a}
\]
  
##### Sum of Squares Between Treatments
Now let's look at the other term in the total sum of squares, the sum of squares between treatments.
  
Suppose there were no differences in between $a$ treatment means. We could use the variation in treatment group averages from the grand mean to estimate the variability in the population, $\sigma^2$:
\[
\frac{SS_{Treatments}}{a-1} = \frac{n\sum_{i=1}^n(\bar y_{i.} - \bar y_{..})^2}{a-1}
\]
  
The ANOVA identity, $SS_{Total} = SS_{Treatments} + SS_{Error}$, provides us with **two estimates of $\mathbf{\sigma^2}$**: one based on the variability **between treatments** and one based on the variability **within treatments**. IF there is no difference in the treatment means, these two estimates should be very similar, and if they are not, **we suspect that the observed difference must be caused by differences in treatment means**.
  
The quantities
\[
\begin{aligned}
MS_{Treatments} =& \frac{SS_{Treatments}}{a - 1}\ \text{, and}\\
MS_{Error} =& \frac{SS_{Error}}{N - a}
\end{aligned}
\]
  
are called **mean squares**. We now examine the **expected values** of these mean squares. Consider
\[
\begin{aligned}
E(MS_E) &= E\biggr(\frac{SS_E}{N-a}\biggl) \\
&= \frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^n(y_{ij} - \bar y_{i.})^2\biggl]\\
&=\frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^n(y_{ij}^2 - 2y_{ij}\bar y_{i.} + \bar y_{i.}^2)\biggl]\\
&=\frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^ny_{ij}^2 - 2n\sum_{i=1}^a\bar y_{i.}^2 + n\sum_{i=1}^a\bar y_{i.}^2\biggl]\\
&=\frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^ny_{ij}^2-\frac{1}{n}\sum_{i=1}^a\sum_{j=1}^ny_{i.}^2\biggl]
\end{aligned}
\]
  
Now replace $y_{ij}$ with $\mu + \tau_i + \epsilon_{ij}$, and we obtain
\[
E(MS_E) = \frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^n(\mu + \tau_i + \epsilon_{ij})^2 - \frac{1}{n}\sum_{i=1}^a\biggr(\sum_{j=1}^n \mu + \tau_i + \epsilon_{ij}\biggl)^2\biggl]
\]
  
Look within the brackets: we see that when squaring and taking the expectation of terms involving $\epsilon_{ij}^2$ and $\epsilon_{i.}^2$ are replaced by $\sigma^2$ and $n\sigma^2$, respectively:
\[
\begin{aligned}
E(MS_E) &= \frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^n(\mu + \tau_i + \epsilon_{ij})^2 - \frac{1}{n}\sum_{i=1}^a\biggr(\sum_{j=1}^n \mu + \tau_i + \epsilon_{ij}\biggl)^2\biggl]\\
&=\frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^n(\mu^2 + \tau_i^2 + \epsilon_{ij}^2 + 2\mu\tau_i + 2\mu\epsilon_{ij} + 2\tau_i\epsilon_{ij}) - \frac{1}{n}\sum_{i=1}^a\biggr(n\mu + n\tau_i + \epsilon_{i.}\biggl)^2\biggl]\\
&=\frac{1}{N-a}E\biggr[\sum_{i=1}^a(n\mu^2 + n\tau_i^2 + \sum_{j=1}^n\epsilon_{ij}^2 + 2n\mu\tau_i + 2\mu\epsilon_{i.} + 2\tau_i\epsilon_{i.}) - \frac{1}{n}\sum_{i=1}^a\biggr((n\mu)^2 + (n\tau_i)^2 + \epsilon_{i.}^2 + 2n^2\mu\tau_i + 2n\mu\epsilon_{i.} + 2n\tau_i\epsilon_{ij}\biggl)\biggl]\\
&=\frac{1}{N-a}E\biggr[(an\mu^2 + n\sum_{i=1}^a\tau_i^2 + \sum_{i=1}^a\sum_{j=1}^n\epsilon_{ij}^2 + 0 + 2\mu\epsilon_{..} + 0) - \biggr(an\mu^2 + n\sum_{i=1}^a\tau_i^2 + \frac{1}{n}\sum_{i=1}^a\epsilon_{i.}^2 + 0 + 2\mu\epsilon_{..} + 0\biggl)\biggl]\\
&=\frac{1}{N-a}E\biggr[an\mu^2 + n\sum_{i=1}^a\tau_i^2 + \sum_{i=1}^a\sum_{j=1}^n\epsilon_{ij}^2 + 2\mu\epsilon_{..} - an\mu^2 - n\sum_{i=1}^a\tau_i^2 - \frac{1}{n}\sum_{i=1}^a\epsilon_{i.}^2 - 2\mu\epsilon_{..}\biggl]\\
&=\frac{1}{N-a}E\biggr[\sum_{i=1}^a\sum_{j=1}^n\epsilon_{ij}^2 - \frac{1}{n}\sum_{i=1}^a\epsilon_{i.}^2 \biggl]\\
&=\frac{1}{N-a}[\sum_{i=1}^a\sum_{j=1}^nE(\epsilon_{ij}^2) - \frac{1}{n}\sum_{i=1}^aE(\epsilon_{i.}^2)]\\
&=\frac{1}{N-a}[anE(\epsilon_{ij}^2) - \frac{an}{n}E(\epsilon_{ij}^2)]\\
&=\frac{1}{N-a}[an\sigma^2 - a\sigma^2]\\
&=\sigma^2
\end{aligned}
\]
  
In conclusion, we have just shown that the expected value of the mean square of error is an unbiased estimator of the population variance, that is,
\[
E(MS_E) = \sigma^2
\]
  
Similarly, the expected value of the mean square of treatment is
\[
E(MS_{treatment}) = \sigma^2 + \frac{n\sum_{i=1}^a\tau_i}{a-1}
\]
  
Thus, $MS_E=SS_E/(N-a)$ estimates $\sigma^2$, and
  
* if there are **no differences in treatment means**, then $MS_{treatments} = SS_{treatments}/(a-1)$ also estimates $\sigma^2$ (because $\tau_i=0$)
* if there **are differences in treatment means**, then the expected value of treatment means is greater than $\sigma^2$.
  
Therefore, the hypothesis test of no difference in treatment means has to compare $MS_{treatments}$ to $MS_E$.
  
#### Statistical Analysis
We can now explore a formal test for the null hypothesis of no difference in treatment means.
  
First, recall that we have assumed
\[
\epsilon_{ij} \sim iid\ N(0,\sigma^2)
\]
  
which implies
\[
y_{ij} \sim iid\ N(\mu + \tau_i,\sigma^2)
\]
  
Thus, $SS_T$ is a sum of squares in normally distributed random variables; consequently, it can be shown that $SS_T/\sigma^2$ is distributed as a **chi-square with N - 1 degrees of freedom**. Furthermore, we can show that
\[
\begin{aligned}
SS_E/\sigma^2 &\sim \chi^2_{N-a}\\
SS_{treatment}/\sigma^2 &\sim \chi^2_{a-1}
\end{aligned}
\]
  
if the null hypothesis $H_0: \tau_i = 0 \forall i$ is true.
  
However, the three sums of squares are not necessarility **independent** because $SS_{treatments} + SS_E = SS_T$. But we need to establish that $SS_E$ and $SS_{treatments}$ are independent; thus we have **Cochran's Theorem**:
  
Let $Z_i\sim NID(0,1)$ for $i=1,2,..,\nu$ and
\[
\sum_{i=1}^{\nu} Z_i^2 = Q_1 + Q_2 + ... + Q_s
\]
  
where $s \le \nu$, and $Q_i$ has $\nu_i$ degrees of freedom ($i = 1,2,...,s$). Then $Q_1,Q_2,...,Q_S$ are independent chi-square random variables with $\nu_1,\nu_2,...,\nu_s$ degrees of freedom, **if and only if**
\[
\nu = \nu_1 + \nu_2 + ... + \nu_s
\]
  
By Cochran's Theorem, because the degrees of freedom for $SS_E$ and $SS_{treatment}$ add to the total degrees of freedom ($N-1$), this implies that $SS_E/\sigma^2$ and $SS_{treatment}/\sigma^2$ are independently distributed chi-square random variables. Therefore, if the null hypothesis of no difference in treatment means is true, the ratio
\[
F_0 = \frac{SS_{treatment}/(a-1)}{SS_E/(N-a)} = \frac{MS_{treatment}}{MS_E}
\]
  
is distributed as *F* with *a - 1* and *N - 1* degrees of freedom. This is the **test statistic** for the hypothesis of no difference in treatment means.
  
If the null hypothesis is false, the expected value of $MS_{treatment}$ is greater than $\sigma^2$. Therefore, under the **althernative hypothesis**, the expected value of the numerator of the test statistic is greater than the expected value of the denominator, and **we should reject $\mathbf{H_0}$ on values that are too large**.
  
Let's go back to our example. First, create the ANOVA table using the statistical package as part of base R.
  
```{r, echo = T}
mod1 <- lm(EtchRate ~ Power, data = dat0)
anova1 <- anova(mod1)
print(anova1)
```
  
Now, calculate the test statistic by hand.
  
```{r, echo = F}
# Find Sum of Squares
  ## Grand Mean
  y..bar <- mean(dat0$EtchRate)
  
  ## Sum across replicates within treatment groups
  level_i <- data.frame(
    summarize(
      group_by(dat0,Power)
      ,yi. = sum(EtchRate)
      ,n = length(EtchRate)
      ,yi.bar = yi./n
      ,yij_y..bar_sq = sum((EtchRate - y..bar)^2)
    )
  )
  
  ## Find number of observations within each treatment group
  n <- unique(level_i$n)
  
  ## Find the number of treatment groups
  a <- NROW(level_i)
  
  ## Calculate SS Total and SS Treatment
  SS_trt <- n*sum((level_i$yi.bar - y..bar)^2)
  SS_tot <- sum(level_i$yij_y..bar_sq)

  ## Back-calculate SS Error
  SS_err <- SS_tot - SS_trt
  
# Degrees of Freedom
  DF_trt <- a - 1
  DF_err <- a*n - a
  
# Mean Squares
  MS_trt <- SS_trt/DF_trt
  MS_err <- SS_err/DF_err

# Test Statistic
  F_test <- MS_trt/MS_err
  
# Critical F-value
  F_crit <- qf(0.05,DF_trt,DF_err,lower.tail = F)

# P-value
  p_value <- pf(q = F_test,df1 = DF_trt,df2 = DF_err,lower.tail = F)

# Table
Anova_by_hand <- as.data.frame(array(
  c(
    "Power","Error","Total",
    SS_trt,SS_err,SS_tot,
    DF_trt,DF_err,DF_trt + DF_err,
    MS_trt,MS_err,"-",
    F_test,"-","-",
    p_value,"-","-"
  )
  ,dim = c(3,6)
))
colnames(Anova_by_hand) <- c("Source fo Variation",
                             "Sum of Squares",
                             "Degrees of Freedom",
                             "Mean Squares",
                             "Test Statistic",
                             "P-value")
print(Anova_by_hand)
```
  
##### Manual Computations
We typically define the sum of squares in terms of **averages**; that is
\[
SS_{treatments} = n\sum_{i=1}^a(\bar y_{i.} - \bar y_{..})^2
\]
  
but we developed the computing formulas using **totals**. For example,
\[
SS_{treatments} = \frac{1}{n}\sum_{i=1}^a y_{i.}^2 - \frac{y_{..}^2}{N}
\]
  
Totals are not as subject to rounding error as averages, though we generally don't need to worry when using statistical software.
  
### 3.3.3 Estimation of the Model Parameters
We now present estimators for the parameters in the single-factor model
\[
y_{ij} = \mu + \tau_i + \epsilon_{ij}
\]
  
and confidence intervals on the treatment means. We will prove later that reasonable estimates of the overall mean and treatment effects are given by
\[
\begin{aligned}
\hat\mu &= \bar y_{..}\\
\hat\tau_i &= \bar y_{i.} - \bar y_{..},\ i=1,2,...,a
\end{aligned}
\]
  
A **confidence interval** of the estimate of the ith treatment mean ($\hat\mu_i$) is easily determined. The mean of the ith treatment is
\[
\mu_i = \mu + \tau_i
\]
  
A **point estimator** of $\mu_i$ would be $\hat\mu_i=\hat\mu + \hat\tau_i=\bar y_{i.}$. If we assume that the errors are normally distributed, each treatment average $\bar y_{i.}$ is distributed $NID(\mu_i,\sigma^2/n)$. Thus, if $\sigma^2$ were known, we could use the normal distribution to define the confidence interval. Using the $MS_E$ as an **estimator of $\mathbf{\sigma^2}$**, we would base the confidence interval on the **t distribution**. Therefore, a $100(1-\alpha)$ percent confidence interval on the ith treatment mean $\mu$ is
\[
\bar y_{i.} - t_{\alpha/2,N-a}\sqrt{\frac{MS_E}{n}} \le \mu_i \le \bar y_{i.} + t_{\alpha/2,N-a}\sqrt{\frac{MS_E}{n}}
\]
  
We can also calculate the confidence interval for the **difference in two treatment means** as
\[
\bar y_{i.} - \bar y_{j.} - t_{\alpha/2,N-a}\sqrt{\frac{2MS_E}{n}} \le \mu_i - \mu_{j.} \le \bar y_{i.} - \bar y_{j.} + t_{\alpha/2,N-a}\sqrt{\frac{2MS_E}{n}}
\]
  
This is a **one-at-a-time** confidence interval. However, in many cases we may wish to calcualte several confidence intervals, one for each mean or difference in means. If there are $r$ such $100(1-\alpha)$ percent confidence intervals of interest, the probability that all $r$ intervals are **simultaneously** correct is $1-r\alpha$. The probability $r\alpha$ is called the **experimentwise error rate**. This can get big really quick.
  
An alternative is to use the **Bonferroni Method** by replacing $\alpha/2$ in the one-at-a-time confidence intervals with $\alpha/(2r)$, which allows us to construct a set of $r$ simultaneous confidence intervals on treatment means or differences in treatment means for which the **overall confidence is at least $\mathbf{100(1-\alpha)}$ percent.
  
### 3.3.4 Unbalanced Data
In some single-factor experiments, the **number of observations taken within each treatment** may be different, a design which is **unbalanced**.
  
Let's look at the effect of the sum of squares. Let $n_i$ observations be taken under treatment $i$, $i = 1,2,...,a$ and $N = \sum_{i=1}^a n_i$. The **manual computations** for $SS_T$ and $SS_{treatments}$ are now slightly different:
\[
\begin{aligned}
SS_T &= \sum_{i=1}^a\sum_{j=1}^n y_{ij}^2 - \frac{y_{..}^2}{N},\\
SS_{treatments} &= \sum_{i=1}^a\frac{y_{i.}^2}{n_i} - \frac{y_{..}^2}{N}
\end{aligned}
\]
  
No other changes are required in the analysis of variance.
  
There are two advantages in choosing a **balanced** design:
  
1. The test statistic is relatively insensitive to small departures from the assumption of equal variances for the $a$ treatments if the sample sizes are equal; that is not the case for unequal sample sizes.
2. The **power is maximized** if the sample sizes are equal.
  
## 3.4 Model Adequacy Checking
The use of partitioning the sum of squares to test formally for no differences in treatment mean requires that certain assumptions be satisfied. Specifically,
  
* The observations are adequately described by the model $y_{ij} = \mu + \tau_i + \epsilon_{ij}$
* The errors are normally and identically distributed with mean zero and constant but unknown variance $\sigma^2$.
  
If these assumptions are valid, the analysis of variance procedure is an exact test of the hypothesis of no difference in treatment means. But, these assumptions need to be checked. Violations of these assumptions can be easily investigated by the examination of **residuals**. We define the residual for observation $j$ in treatment $i$ as
\[
e_{ij} = y_{ij} - \hat y_{ij}
\]
  
where $\hat y_{ij}$ is the estimate of the corresponding observation $y_{ij}$ obtained by
\[
\begin{aligned}
\hat y_{ij} &= \hat\mu + \hat\tau_i\\
&=\bar y_{..} + (\bar y_{i.} - \bar y_{..})\\
&=\bar y_{i.}
\end{aligned}
\]
  
If the model is adequate, the residuals should be **structureless**, i.e., should not contain any obvious patterns.
  
### 3.4.1 The Normality Assumption
A check of the normality assumption could be made by plotting a **histogram of the residuals**; if the normality assumption holds, the plot should look like a sample from normal distribution centered at zero. However, with small sample sizes this appearance is easily distorted, and visually moderate departure from normality does not necessarily imply a serious violation of the assumptions.
  
Consider a **normal probability plot** of the residuals. If the underlying distribution of the errors is normal, this plot will resemble a straight line. Note that more emphasis should be placed on the central values of the plot than on the extremes.
  
In the plot below, the tendency of the normal quantile-quantile plot to bend slightly **down on the left side** and **upward slightly on the right side** implies that the tails of the error are somewhat **thinner** than what would be anticipated in a normal distribution

  
```{r, echo = T}
# Normal Quantile-Quantile Plot - residuals
qqnorm(mod1$residuals,datax = T);qqline(mod1$residuals,datax = T)
```
  
We say that the analysis of variance is **robust** to the normality assumption because the *F* test is only slightly affected by depatures from normlity. In general, moderate departures from normality are of little concern in the **fixed effects** analysis of variance. An error distribution with thinner or thicker tails is more concerning than a skewed distribution. The **random effects model** is more severly affected by nonnormality.
  
An **outlier** is a residual that is very much larger than the others, and these can seriously **distort the analysis of variance**. We can roughly check for outliers by examining the **standardized residuals**
\[
d_{ij} = \frac{e_{ij}}{\sqrt{MS_E}}
\]
  
These should be approximately **normal** with mean zero and variance of one. A residual bigger than 3 or 4 standard deviations away from zero is a potential outlier.
  
### 3.4.2 Plot of Residuals in Time Series
To detect **correlation** between residuals, plot residuals in order of data collection. If you see runs of positive or negative residuals, then the errors may be violating the **independence assumption** on the errors. Proper randomization of the experiment is important in obtaining independence.
  
**Nonconstant variance** is another violation of assumptions, and is generally seen as more spread at one end than the other in a plot of redisuals versus time. For example, say the skill of the experimenter, operator, or subject changed over time.
  
### 3.4.3 Plot of Residuals Versus Fitted Values.
If the model is correct and assumptions are satisfied, the residuals should be unrelated to any other variable, including the predicted response. The plot of residuals versus fitted values for the data are below. No unusual structure is apparent.
  
```{r, echo = T}
plot(mod1,which = 1)
```
  
Here is where we might see **nonconstant variance**, when the variance of the observations increases as the magnitude of the observation increases:
  
* This is the case when the error or background noise is a constant percentage of the size of the observation like, for example, with many measuring instruments.
* This can also happen when the data follow a nonnormal, skewed distribution, because in skewed distribution the **variance tends to be a function of the mean**.
  
The *F* test is only affected by violation of the homogeneity of variances assumption if the **balanced** fixed effects model. However, in **unbalanced** designs or in **cases where one variance is much larger than the others**, is more serious:
  
* If the factor levels with large variances also have the smaller sample sizes, the actual **Type I Error** is larger than anticipated (confidence intervals have **lower actual confidence levels** than specified).
* If the the factor levels with larger variances also have the larger sample sizes, the **significance levels** are smaller than anticipated (**confidence levels are higher**).
  
For random effects models, unequal error variances can significantly disturb inferences on variance components even when designs are balanced.
  
To deal with nonconstant variance, we can usually apply a **variance-stabilizing transformation**. Conclusions of the analysis of variance will then apply to the **transformed population**.
  
* If observations follow the **Poisson Distribution**, the **square root transformation**, $y_{ij}^{\star} = \sqrt{y_)_{ij}}$ or $y_{ij}^{\star} = \sqrt{1 + y_{ij}}$ would be used.
* If observations follow follow the **lognormal distribution**, then the **logarithmic transformation** $y_{ij}^{\star} = log(y_{ij})$ is appropriate.
* For **binomial data** expressed as fractions, the **arcsin transformation** $y_{ij}^{\star} = arcsin \sqrt{y_{ij}}$.
  
When there is no obvious transformation, the experimenter usually *empirically* seeks a transformation that equalizes the variance regardless of the value of the mean.
  
##### Statistical Tests for Equality of Variance
We can use formal tests of the hypotheses
\[
\begin{aligned}
H_0:& \sigma_1^2 = \sigma_2^2 = ... = \sigma_a^2\\
H_1:&\text{above not true for at least one }\sigma_i^2
\end{aligned}
\]
  
A widely used procedure is **Bartlett's test**, which involves computing a statistic whose distribution is closely approximated by the chi-square distribution with $a - 1$ degrees of freedom when the *a* random samples are drawn from independent normal populations. The test statistic is
\[
\chi_0^2 = 2.3026\frac{q}{c}
\]
  
where
\[
\begin{aligned}
q &= (N-a)log_{10}(S_p^2) - \sum_{i=1}^a(n_i - 1)log_{10}(S_i^2)\\
c &= 1 + \frac{1}{3(a-1)}(\sum_{i=1}^a(n_i - 1)^{-1} - (N-a)^{-1})\\
S_p^2 &= \frac{\sum_{i=1}^a(n_i - 1)S_i^2}{N-a}
\end{aligned}
\]
  
and $S_i^2$ is the sample variance of the ith population.
  
* *q* is **large** when the sample variances $S_i^2$ **differ greatly**.
* *q* is **equal to zero** when all $S_i^2$ are **equal**.
  
Therefore, we should reject $H_0$ on values of $\chi_0^2$ that are too large, when
\[
\chi_0^2 > \chi_{\alpha,\alpha-1}^2
\]
  
**Bartlett's test** is very sensitive to the normality assumption, and so can only be used when the validity of this assumption is not in doubt.
  
The result of Bartlett's test is provided below. We fail to reject the null hypothesis of constant variance.
```{r, echo = T}
# Bartlett's Test in R
mod1$call
bart.test <- bartlett.test(EtchRate ~ Power, data = dat0)
bart.test
## Matches example in book.
crit.value <- qchisq(0.05,a-1,lower.tail = F);crit.value
p.value    <- pchisq(bart.test$statistic,a-1,lower.tail = F);p.value
```
  
The **modified Levene test** can be used in situations when the nonnormality assumption is not met (and Bartlett's test cannot be used). The modified Levene's test uses the absolute deviation of observations $y_{ij}$ in each treatment from the treatment median $\tilde{y_{ij}}$:
\[
d_{ij} = |y_{ij} - \tilde{y_{ij}}|\ \text{for }i=1,...a\text{ and }j=1,...,n_i
\]
  
The modified Levene's test evaluates whether the means of these deviations are equal for all treatments. If the mean deviations are equal, the variations of the observations in all treatments will be the same. The test statistic is the usual ANOVA *F* statistic for test equality of means applied to the absolute deviations.
  
```{r, echo = T}
# New Data: Peak Discharge by Estimation Methods
dat1 <- as.data.frame(
  array(
    c(
      rep(c(1,2,3,4),6),
    0.34,0.91,6.31,17.15,
    0.12,2.94,8.37,11.82,
    1.23,2.14,9.75,10.95,
    0.70,2.36,6.09,17.20,
    1.75,2.86,9.82,14.35,
    0.12,4.55,7.24,16.82
    )
    ,dim = c(4*6,2)
  )
)

colnames(dat1) <- c("EstimationMethod","PeakDischarge")

# Calculate mean, median, and standard deviation

level_i <- data.frame(
  summarize(
    group_by(dat1,EstimationMethod)
    ,n_i       = length(PeakDischarge)
    ,yi.bar    = mean(PeakDischarge)
    ,yi.median = median(PeakDischarge)
    ,Si        = sd(PeakDischarge)
  )
)

# Calculate Deviations for Modified Levene's test
dat1_median <- merge(dat1,level_i,by="EstimationMethod")
dat1_median$d_ij <- abs(dat1_median$PeakDischarge - dat1_median$yi.median)

# Analysis of Variance
dat1_median$EstimationMethod <- as.factor(dat1_median$EstimationMethod)
mod2 <- lm(PeakDischarge ~ EstimationMethod, data = dat1_median)
anova(mod2)

# Look at normal QQ plot
qqnorm(mod2$residuals,datax = T);qqline(mod2$residuals,datax = T)

# Look at plot of residuals versus fitted values
plot(mod2, which = 1)

```
  
##### Empirical Selection of a Transformation
Let $E(y) = \mu$ be the mean of *y*, and suppose that the standard deviation of *y* is proportional to a power of the mean of *y* such that
\[
\sigma_y \propto \mu^{\alpha}
\]
  
We want to find a transformation on *y* that yields a constant variance. Suppose that the transformation is a power of the original data, say
\[
y^{\star} = y^{\lambda}
\]
  
Then it can be shown that
\[
\sigma_{y^{\star}} \propto \mu^{\lambda + \alpha-1}
\]
  
If we set $\lambda = 1 - \alpha$, the variance of the transformed data $y^{\star}$ is constant. See a list of common transformations in the table below. Note that $\lambda = 0$ implies the log transformation. These transformations are ordered by an increase in **strength* (the amount of curvature it induces). Transformations have little effect unless the ratio of $y_{max}/y_{min}$ is greater than 2 or 3.
  
We can usually empirically estimate $\alpha$ from the data. Because the ith treatment combination $\sigma_{y_i} \propto \mu_i^{\alpha} = \theta\mu_i^{\alpha}$, where $\theta$ is a **constant of proportionality**, we may take logs to obtain
\[
log(\sigma_{y_i}) = log(\theta) + \alpha log(\mu_i)
\]
  
Therefore, a **plot of $\mathbf{log(\sigma_{y_i})}$ versus $\mathbf{log(\mu_{i})}$** would be a straight line with slope of $\alpha$. Because we don't know $\sigma_{y_i}$ or $\mu_i$, we subsitute reasonable estimates of them ($S_i$ and $\bar y_{i.}$) into the equation and fit the regression, estimating $\alpha$, the slope.
  
```{r, echo = F}
TransformationTab <- as.data.frame(
  array(c(
    "$\\sigma_y \\propto\\ \\text{constant}$",
    "$\\sigma_y \\propto\\ \\mu^{1/2}$",
    "$\\sigma_y \\propto\\ \\mu$",
    "$\\sigma_y \\propto\\ \\mu^{3/2}$",
    "$\\sigma_y \\propto\\ \\mu^{2}$",
    
    "0","1/2","1","3/2" ,"2" ,
    "1","1/2","0","-1/2","-1",
    
    "No transformation","Square Root","Log","Reciprocal of square root","Reciprocal",
    
    "","Poisson (count) data","","",""
  ),dim = c(5,5))
)

knitr::kable(
  TransformationTab,
  row.names = FALSE,
  col.names = c("Relationship Between $\\sigma_y$ and $\\mu$",
                "$\\alpha$","$\\lambda = 1 - \\alpha$",
                "Transformation","Comment"))
```
  
Let's take a look at the peak discharge data and the possibility of using a variance-stabilizing transformation. First, we plot $log(S_i)$ versus $log(\bar y_{i.})$.
  
```{r, echo = T}
varstab.fit <- lm(I(log(Si)) ~ I(log(yi.bar)), data = level_i)
varstab.fit
plot(x = log(level_i$yi.bar),y = log(level_i$Si))
```
  
The slope, our estimate of $\alpha$, is `r varstab.fit$coefficients[2]`, which is almost 1/2. From the table of transformations, a square root transformation may be appropriate. The analysis of variance table is provided below.
  
```{r,echo = T}
mod3 <- lm(I(sqrt(PeakDischarge)) ~ EstimationMethod, data = dat1_median)
anova(mod3)
```
  
But we need to **manually decrease the degrees of freedom by 1** and re-run the F-test; this is to account for the use of the data to estimate the transformation parameter $\alpha$. The updated results are below.
  
```{r, echo = T}
# Establish table to fill in
anova3a <- data.frame(anova(mod3))

# Re-calculate MSE and F test, update p-value
anova3a["Residuals","Df"] <- anova3a["Residuals","Df"] - 1
anova3a["Residuals","Mean.Sq"] <- 
  anova3a["Residuals","Sum.Sq"]/anova3a["Residuals","Df"]
anova3a["EstimationMethod","F.value"] <-
  anova3a["EstimationMethod","Mean.Sq"]/anova3a["Residuals","Mean.Sq"]
anova3a["EstimationMethod","Pr..F."] <- pf(
  anova3a["EstimationMethod","F.value"],
  anova3a["EstimationMethod","Df"],
  anova3a["Residuals","Df"],
  lower.tail = F
)

# Final Analysis of Variance Table
anova3a
```
  
The plot of residuals versus fitted values is shown below. We see that the variance appears much more constant.
  
```{r, echo  T}
plot(mod3,which = 1)
```
  
### Plots of Residuals Versus Other Variables
The residuals should be plotted against any other variables that might possibly affect the response. Patters in such residual plots suggest that the variable affects the response, and the variable should either be controlled more carefully in future experiments or included in the analysis.
  
## 3.5 Practical Interpretation of Results.
### 3.5.1 A Regression Model
Factors involved in an experiment can be either:
  
* **quantitative**, whose levels can be associated with points on a numerical scale, or
* **qualitative**, whose levels cannot be arranged in order of magnitude.
  
Operators, batches of raw material, and shifts are typical qualitative factors because there is no reason to rank them in any particular numerical order. The analysis of variance treat the design factor as if it were qualitive or categorical (without numerical meaning). If the factor really is qualitative, responses for a subsequent run at intermediate levels are not meaningful (for example, we don't really want to predict responses for an operator between the second and third in the experiment).
  
However, if the levels can be ordered numerically, say 1.0, 2.0, and 3.0, we may want to predict the response at 2.5. Thus, the experimenter is frequently interested in developing an **interpolation equation** for the response variable in the experiment. This equation is an **empirical model** of the process that has been studied.
  
We can see from the scatterplot of Power against Etch Rate that there is a strong relationship between the two variables. At first we try fitting a **linear model**:
\[
y = \beta_0 + \beta_1x + \epsilon
\]
  
where $\beta_0$ and $\beta_1$ are unknown parameters to be estimated and $\epsilon$ is a random error term. We use the **method of least squares** to estimate model parameters. In the linear model, the least squares fit in our example is
  
\[
\hat y = 137.62 + 2.527x
\]
  
```{r, echo = T}
mean(dat0$EtchRate) - 137.62
mean(as.numeric(dat0$Power))
lm(EtchRate ~ I(as.numeric(Power)), data = dat0)
  ## Can't replicate the model in the book yet, unsure why

dat0_wide <- dcast(data = dat0
                   ,Power ~ Replication
                   ,value.var = "EtchRate")

```
  
```{r, echo = T}
# By hand
EtchRate_i <- data.frame(
  summarize(
    group_by(dat0,Power)
    ,yi.bar = mean(EtchRate)
  )
)

y..bar <- mean(dat0$EtchRate)

EtchRate_i$Estimate <- EtchRate_i$yi.bar - y..bar

EtchRate_i

# In R
## Need to demean the response to get estimates of treatment effects
mod1_numeric <- lm(I(EtchRate - mean(EtchRate)) ~ 0 + Power, data = dat0)
  # Supress intercept so that we can estimate yi.bar for all levels
data.frame(summary(mod1_numeric)$coefficients)
```
  
This linear model is shown in the plot below. We can see that it does not perform well at high levels of Power.
  
```{r, echo = T}
# Plot data points and trendline
ggplot(data = dat0,mapping = aes(x = as.numeric(Power),
                                 y = EtchRate)) + 
  geom_point() +
  geom_smooth(method = "lm")
```
  
We can try to improve on the model by fitting a **quadratic model** and adding a quadratic term in *x*. The resulting model fit is
  
```{r, echo = T}
# Fit the quadratic model
lm(EtchRate ~ I(as.numeric(Power)) + I(as.numeric(Power)^2), data = dat0)
```
  
```{r, echo = T}
# Plot data points and trendline
ggplot(data = dat0,mapping = aes(x = as.numeric(Power),
                                 y = EtchRate)) + 
  geom_point() +
  stat_smooth(method = "lm")
```






















